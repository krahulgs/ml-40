{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "382SPBHUGUco"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "dataset = pd.read_excel(\"data.xlsx\")\n",
        "\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "\n",
        "X.columns = [\n",
        "    \"Age\",\"gender\",\"city\",\"state\",\"year\",\"previous_claims\",\n",
        "    \"vehicle_type\",\"Customer_tenure\",\"Policy_source\",\"cp_code\",\n",
        "    \"Occupation_Type\",\"DIY_vs_CC_behavior\",\"renewal_count\",\"Claim_status\"\n",
        "]\n",
        "\n",
        "# Binary encoding\n",
        "binary_cols = [\"gender\",\"vehicle_type\",\"DIY_vs_CC_behavior\"]\n",
        "\n",
        "binary_encoders = {}\n",
        "\n",
        "for col in binary_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    binary_encoders[col] = le\n",
        "\n",
        "\n",
        "#for col in binary_cols:\n",
        " #   X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Embedding columns\n",
        "embed_cols = [\"city\", \"state\", \"Occupation_Type\", \"Policy_source\"]\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "embed_encoders = {}\n",
        "\n",
        "for col in embed_cols:\n",
        "    oe = OrdinalEncoder(\n",
        "        handle_unknown=\"use_encoded_value\",\n",
        "        unknown_value=-1\n",
        "    )\n",
        "    X[[col]] = oe.fit_transform(X[[col]])\n",
        "    embed_encoders[col] = oe\n",
        "\n",
        "# Numeric scaling\n",
        "numeric_cols = [\n",
        "    \"Age\",\"year\",\"previous_claims\",\"Customer_tenure\",\n",
        "    \"cp_code\",\"Claim_status\",\"renewal_count\"\n",
        "]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Dense, Flatten,\n",
        "    Concatenate, Dropout, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "2EDyC5ThGosY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = []\n",
        "embeddings = []\n",
        "\n",
        "# Embeddings\n",
        "for col in embed_cols:\n",
        "    vocab_size = X[col].nunique() + 1\n",
        "    embed_dim = min(50, vocab_size // 2)\n",
        "\n",
        "    inp = Input(shape=(1,), name=f\"{col}_input\")\n",
        "    emb = Embedding(vocab_size, embed_dim, name=f\"{col}_emb\")(inp)\n",
        "    emb = Flatten()(emb)\n",
        "\n",
        "    inputs.append(inp)\n",
        "    embeddings.append(emb)\n",
        "\n",
        "# Numeric & binary inputs\n",
        "num_input = Input(shape=(len(numeric_cols) + len(binary_cols),), name=\"num_input\")\n",
        "inputs.append(num_input)\n",
        "\n",
        "x = Concatenate()(embeddings + [num_input])\n",
        "\n",
        "# Deep layers\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "\n",
        "output = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n"
      ],
      "metadata": {
        "id": "tcaNdK6pGvDP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\n",
        "        tf.keras.metrics.AUC(name=\"auc\"),\n",
        "        tf.keras.metrics.Precision(name=\"precision\"),\n",
        "        tf.keras.metrics.Recall(name=\"recall\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "jdfa0bBsG0JL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "class_weights = dict(enumerate(class_weights))\n"
      ],
      "metadata": {
        "id": "4egDlbgIG3ci"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_auc\",\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        mode=\"max\"\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    [\n",
        "        X_train[col] for col in embed_cols\n",
        "    ] + [X_train[numeric_cols + binary_cols].values],\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hww2k0yoG67N",
        "outputId": "892e698b-ed5a-4fea-abb7-8ad03f101869"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - auc: 0.8457 - loss: 0.4522 - precision: 0.6545 - recall: 0.7777 - val_auc: 0.9998 - val_loss: 0.2911 - val_precision: 0.9914 - val_recall: 0.9858\n",
            "Epoch 2/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - auc: 0.9982 - loss: 0.0497 - precision: 0.9848 - recall: 0.9892 - val_auc: 1.0000 - val_loss: 0.1486 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - auc: 0.9999 - loss: 0.0169 - precision: 0.9929 - recall: 0.9969 - val_auc: 1.0000 - val_loss: 0.0816 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - auc: 0.9999 - loss: 0.0116 - precision: 0.9929 - recall: 0.9973 - val_auc: 1.0000 - val_loss: 0.0441 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - auc: 1.0000 - loss: 0.0061 - precision: 0.9981 - recall: 0.9980 - val_auc: 1.0000 - val_loss: 0.0243 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - auc: 1.0000 - loss: 0.0045 - precision: 0.9976 - recall: 0.9994 - val_auc: 1.0000 - val_loss: 0.0131 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - auc: 1.0000 - loss: 0.0042 - precision: 0.9994 - recall: 0.9979 - val_auc: 1.0000 - val_loss: 0.0074 - val_precision: 1.0000 - val_recall: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_input = {\n",
        "    \"Age\": 63,\n",
        "    \"gender\": \"M\",\n",
        "    \"city\": \"Ranbirsinghpora\",\n",
        "    \"state\": \"JAMMU & KASHMIR\",\n",
        "    \"year\": 2022,\n",
        "    \"previous_claims\": 0,\n",
        "    \"vehicle_type\": \"N\",\n",
        "    \"Customer_tenure\": 1,\n",
        "    \"Policy_source\": \"Dealer\",\n",
        "    \"cp_code\": 21869,\n",
        "    \"Occupation_Type\": \"Self Employeed\",\n",
        "    \"DIY_vs_CC_behavior\": 0,\n",
        "    \"Claim_status\": 0,\n",
        "    \"renewal_count\": 2\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame([raw_input])\n",
        "\n",
        "# Binary encoding\n",
        "# Standardize binary text values first\n",
        "#for col, mapping in STANDARDIZE_MAP.items():\n",
        " #   df[col] = df[col].map(mapping)\n",
        "\n",
        "# Encode binary columns\n",
        "for col in binary_cols:\n",
        "    df[col] = binary_encoders[col].transform(df[col])\n",
        "\n",
        "# Encode embedding columns (SAFE for unseen)\n",
        "for col in embed_cols:\n",
        "    df[[col]] = embed_encoders[col].transform(df[[col]])\n",
        "\n",
        "df[embed_cols] = df[embed_cols] + 1\n",
        "\n",
        "# Scale numeric columns\n",
        "df[numeric_cols] = scaler.transform(df[numeric_cols])\n",
        "\n",
        "# Prepare model inputs\n",
        "model_inputs = (\n",
        "    [df[col].values for col in embed_cols] +\n",
        "    [df[numeric_cols + binary_cols].values]\n",
        ")\n",
        "\n",
        "probability = model.predict(model_inputs)[0][0]\n",
        "\n",
        "print(\"Predicted Probability:\", probability)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u83nHN61H3sw",
        "outputId": "96a88257-6d10-4819-e7ff-42984229f2a9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
            "Predicted Probability: 0.6002482\n"
          ]
        }
      ]
    }
  ]
}